{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FairnessFrameworkModule.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMYZmgQtDqVAsf1WEKO0I7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarveshsuresh/FairnessTest/blob/main/FairnessFrameworkModule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pz8UvSOjqP3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20be721-e887-4315-c814-747a7641c5cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns  \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dfr,target):\n",
        "  df=dfr\n",
        "  cols = list(df.columns)\n",
        "  cols.remove(target)\n",
        "  num_cols = list(df.drop(columns=[target])._get_numeric_data().columns)\n",
        "  cat_cols=list(set(cols) - set(num_cols))\n",
        "  #return(num_cols,cat_cols)\n",
        "\n",
        "  for column in num_cols:\n",
        "    if len(list(df[column].unique()))<10:\n",
        "      df[column]=df[column].astype('str')\n",
        "      df[column].fillna('missing',inplace=True)\n",
        "\n",
        "    else:\n",
        "      df[column].fillna(np.mean(df[column]),inplace=True)\n",
        "\n",
        "  for column in cat_cols:\n",
        "    df[column].fillna('missing',inplace=True)\n",
        "    if len(list(df[column].unique()))>10:\n",
        "      df=df.drop(columns=[column])\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  return(df)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "43xRBjBtqZEh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelling(dfr,target,model,test_size,sens):\n",
        "  df=dfr\n",
        "  cols = list(df.columns)\n",
        "  cols.remove(target)\n",
        "  num_cols = list(df.drop(columns=[target])._get_numeric_data().columns)\n",
        "  cat_cols=list(set(cols) - set(num_cols))\n",
        "  if model =='xgb' or model=='dt' or model=='rf' or model=='lgbm' or model=='catboost':\n",
        "    for column in cat_cols:\n",
        "      encoder=LabelEncoder()\n",
        "      \n",
        "        \n",
        "\n",
        "      df[column] = (encoder.fit_transform(df[column]))\n",
        "      if column==sens:\n",
        "\n",
        "        sencoder=encoder\n",
        "        #print('het')\n",
        "    y=df[target]\n",
        "    df_train,df_test=train_test_split(df,test_size=test_size,stratify=y) #stratified sampling\n",
        "    X,Y=df_train.drop(columns=[target]),df_train[target]\n",
        "    #print(X)\n",
        "\n",
        "    if model=='dt':\n",
        "      clf = tree.DecisionTreeClassifier()\n",
        "      clf = clf.fit(X, Y)\n",
        "      df_test['prediction']=clf.predict(df_test.drop(columns=[target]))\n",
        "      data_crosstab = pd.crosstab(df_test[target],\n",
        "                          df_test['prediction'], \n",
        "                              margins = False)\n",
        "      \n",
        "      #return(data_crosstab)\n",
        "      #print(sencoder.inverse_transform(df_test[sens]))\n",
        "      df_test[sens]=sencoder.inverse_transform(df_test[sens])\n",
        "      acc=accuracy_score(df_test[target],df_test['prediction'])\n",
        "      return(df_test,acc)\n",
        "\n",
        "    if model=='xgb':\n",
        "      params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "      xgb = XGBClassifier(learning_rate=0.08, n_estimators=500, objective='binary:logistic',\n",
        "                    silent=True, nthread=1)\n",
        "      folds = 3\n",
        "      param_comb = len(params)\n",
        "\n",
        "      skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 42)\n",
        "\n",
        "      random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=skf.split(X,Y), random_state=42 )\n",
        "\n",
        "      random_search.fit(X,Y)\n",
        "\n",
        "      df_test['prediction']=random_search.predict(df_test.drop(columns=[target]))\n",
        "      data_crosstab_xgb = pd.crosstab(df_test[target],\n",
        "                          df_test['prediction'], \n",
        "                              margins = False)\n",
        "      \n",
        "      #return(data_crosstab_xgb)\n",
        "      df_test[sens]=sencoder.inverse_transform(df_test[sens])\n",
        "      acc=accuracy_score(df_test[target],df_test['prediction'])\n",
        "      return(df_test,acc)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "  elif model=='logreg' or model=='svm':\n",
        "    stored_column=df[sens]\n",
        "    df=pd.get_dummies(df,columns=cat_cols,drop_first=True)\n",
        "    df[sens]=stored_column\n",
        "    y=df[target]\n",
        "    df_train,df_test=train_test_split(df,test_size=test_size,stratify=y) #stratified sampling\n",
        "    \n",
        "    for column in num_cols:\n",
        "      df_train[column]=(df_train[column]-np.mean(df_train[column]))/np.std(df_train[column])\n",
        "      df_test[column]=(df_test[column]-np.mean(df_train[column]))/np.std(df_train[column])\n",
        "    X,Y=df_train.drop(columns=[target,sens]),df_train[target]\n",
        "\n",
        "    if model=='logreg':\n",
        "      clf=LogisticRegression()\n",
        "      clf.fit(X,Y)\n",
        "      df_test['prediction']=clf.predict(df_test.drop(columns=[target,sens]))\n",
        "      data_crosstab = pd.crosstab(df_test[target],\n",
        "                          df_test['prediction'], \n",
        "                              margins = False)\n",
        "      \n",
        "      #return(data_crosstab)\n",
        "\n",
        "      acc=accuracy_score(df_test[target],df_test['prediction'])\n",
        "      return(df_test,acc)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  #return(df)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lKPLXZxQ18c6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prop_parity(cross,tp,fp,tn,fn):\n",
        "  val=(tp+fp)/(tp+fp+tn+fn)\n",
        "  return(val)\n",
        "\n",
        "def equalized_odds(cross,tp,fp,tn,fn):\n",
        "  val=(tp)/(tp+fn)\n",
        "  return(val)\n",
        "\n",
        "def pred_rate_parity(cross,tp,fp,tn,fn):\n",
        "  val=(tp)/(tp+fp)\n",
        "  return(val)\n",
        "\n",
        "def accuracy_parity(cross,tp,fp,tn,fn):\n",
        "  val=(tp+tn)/(tp+fp+tn+fn)\n",
        "  return(val)\n",
        "\n",
        "def false_negrate_parity(cross,tp,fp,tn,fn):\n",
        "  val=(fn)/(tp+fn)\n",
        "  return(val)\n",
        "\n",
        "def false_posrate_parity(cross,tp,fp,tn,fn):\n",
        "  val=(fp)/(fp+tn)\n",
        "  return(val)\n",
        "\n",
        "\n",
        "def neg_predval_parity(cross,tp,fp,tn,fn):\n",
        "  val=(tn)/(tn+fn)\n",
        "  return(val)\n",
        "\n",
        "def specificity_parity(cross,tp,fp,tn,fn):\n",
        "  val=(tn)/(fp+tn)\n",
        "  return(val)\n",
        "\n"
      ],
      "metadata": {
        "id": "47P3cJy1OJfj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_groups(dataframe,column,metric,actual,preds,pos,neg):\n",
        "  dff=pd.DataFrame()\n",
        "  levells=[]\n",
        "  levels=dataframe[column].unique()\n",
        "  vals=[]\n",
        "  for level in levels:\n",
        "    print(level)\n",
        "    try:\n",
        "      sub=dataframe[dataframe[column]==level]\n",
        "      data_crosstab = pd.crosstab(sub[actual],\n",
        "                              sub[preds], \n",
        "                                margins = False)\n",
        "      print(data_crosstab)\n",
        "      tp=data_crosstab[pos][pos]\n",
        "      tn=data_crosstab[neg][neg]\n",
        "      fp=data_crosstab[pos][neg]\n",
        "      fn=data_crosstab[neg][pos]\n",
        "      vals.append(metric(data_crosstab,tp,tn,fp,fn))\n",
        "      levells.append(level)\n",
        "    except:\n",
        "      pass\n",
        "  dff['level'],dff['metric']=levells,vals\n",
        "    \n",
        "  return(dff)"
      ],
      "metadata": {
        "id": "XFu5e202OVgd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/train_u6lujuX_CVtuZ9i (1).csv')"
      ],
      "metadata": {
        "id": "zjm5aFgdKI5p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Gender'].unique()[0]"
      ],
      "metadata": {
        "id": "hr3bDifxKMXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_groups_multiclass(dataframe,column,metric,reference,actual,preds):\n",
        "  datafr=dataframe\n",
        "  dff=pd.DataFrame()\n",
        "  levells=[]\n",
        "  levels=datafr[column].unique()\n",
        "  vals=[]\n",
        "  if len(levels)>2:\n",
        "    datafr[actual]=np.where(datafr[actual]==reference,datafr[actual],'not '+reference)\n",
        "    datafr[preds]=np.where(datafr[preds]==reference,datafr[preds],'not '+reference)\n",
        "    pos=reference\n",
        "    neg='not '+reference\n",
        "    for level in levels:\n",
        "      print(level)\n",
        "      try:\n",
        "        sub=datafr[datafr[column]==level]\n",
        "        data_crosstab = pd.crosstab(sub[actual],\n",
        "                                  sub[preds], \n",
        "                                    margins = False)\n",
        "        print(data_crosstab)\n",
        "        tp=data_crosstab[pos][pos]\n",
        "        tn=data_crosstab[neg][neg]\n",
        "        fp=data_crosstab[pos][neg]\n",
        "        fn=data_crosstab[neg][pos]\n",
        "        \n",
        "        vals.append(metric(data_crosstab,tp,tn,fp,fn))\n",
        "        levells.append(level)\n",
        "        \n",
        "\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    dff['level'],dff['metric']=levells,vals\n",
        "  else:\n",
        "    pos=reference\n",
        "    neg=datafr[datafr[actual]!=reference][actual].unique()[0]\n",
        "\n",
        "    for level in levels:\n",
        "      try:\n",
        "        print(level)\n",
        "        \n",
        "        sub=datafr[datafr[column]==level]\n",
        "        data_crosstab = pd.crosstab(sub[actual],\n",
        "                                  sub[preds], \n",
        "                                    margins = False)\n",
        "        print(data_crosstab)\n",
        "      \n",
        "        tp=data_crosstab[pos][pos]\n",
        "        \n",
        "        tn=data_crosstab[neg][neg]\n",
        "        fp=data_crosstab[pos][neg]\n",
        "        fn=data_crosstab[neg][pos]\n",
        "        \n",
        "\n",
        "        vals.append(metric(data_crosstab,tp,tn,fp,fn))\n",
        "        \n",
        "\n",
        "        levells.append(level)\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "    dff['level'],dff['metric']=levells,vals\n",
        "    print(dff)\n",
        "  return(dff)\n",
        "\n"
      ],
      "metadata": {
        "id": "yCDaLL03ErNv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def threshold_eval(tdat,threshold,dat,sens):\n",
        "  tdat=tdat.sort_values('metric',ascending=False).reset_index(drop=True)\n",
        "  comp=tdat.loc[0,'metric']\n",
        "  #tdat['difference']=(comp/tdat['metric'])-1\n",
        "  #sub=tdat[tdat['difference']>threshold]\n",
        "  lengths=[]\n",
        "  pvals=[1]\n",
        "\n",
        "\n",
        "  for i in range(len(tdat)):\n",
        "    leng_var=len(dat[dat[sens]==tdat.loc[i,'level']])\n",
        "    lengths.append(leng_var)\n",
        "    if i>0:\n",
        "      leng_fix=len(dat[dat[sens]==tdat.loc[0,'level']])\n",
        "      \n",
        "      \n",
        "\n",
        "      nobs = np.array([leng_fix,leng_var])\n",
        "      count = np.array([tdat.loc[0,'metric']*leng_fix,tdat.loc[i,'metric']*leng_var])\n",
        "      stat, pval = proportions_ztest(count, nobs)\n",
        "      #print((pval))\n",
        "      pvals.append(pval)\n",
        "  tdat['Count']=lengths\n",
        "  tdat['p-value']=pvals\n",
        "  problems=tdat[tdat['p-value']<threshold]\n",
        " \n",
        "\n",
        "\n",
        "  return(tdat,problems)"
      ],
      "metadata": {
        "id": "jsMRjodCRqjt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_modelling(df_test,clffile,target,sens):\n",
        "  clf=pickle.load(open(clffile, \"rb\"))\n",
        "  df_test['prediction']=clf.predict(df_test.drop(columns=[target,sens]))\n",
        "  \n",
        "  acc=accuracy_score(df_test[target],df_test['prediction'])\n",
        "  return(df_test,acc)"
      ],
      "metadata": {
        "id": "2zMEMd2TW8Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_fairness(modelfile,testset,target,sens,metric,pos,neg,threshold):\n",
        "  dataf,acc=simple_modelling(testset,modelfile,target,sens)\n",
        "  tdata=compare_groups(dataf,sens,metric,target,'prediction',pos,neg)\n",
        "  if len(tdata)==0:\n",
        "    Conclusion='Model too simplistic'\n",
        "  else:\n",
        "    sum_dat,prob_dat=threshold_eval(tdata,threshold,dataf,sens)\n",
        "    print('accuracy: ',acc)\n",
        "\n",
        "    if len(sum_dat)==1:\n",
        "      Conclusion='Model too simplistic'\n",
        "    \n",
        "    elif len(prob_dat)==0:\n",
        "      Conclusion='The given model is deemed to be fair towards all levels of '+sens+' at the '+str(threshold*100)+' % level of significance'\n",
        "\n",
        "    else:\n",
        "      Conclusion='The given model is deemed to be unfair towards the following levels of '+sens+' at the '+str(threshold*100)+' % level of significance : '\n",
        "      kk=0\n",
        "      for ii in prob_dat['level']:\n",
        "        kk+=1\n",
        "        Conclusion+=ii\n",
        "        if kk<len(prob_dat):\n",
        "          Conclusion+=', '\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(Conclusion)\n",
        "\n",
        "  lines = ['The Following are the parameters of the fairness-test: ', 'Target variable: '+target,'Model used: '+modelfile,'Sensitive Variable: '+sens,\n",
        "           'Metric used for calculating parity(fairness): '+metric.__name__,'Level of Significance: '+(str(int(threshold*100)))+' %',Conclusion,'Accuracy of the model: '+str(acc)]\n",
        "  with open('report.txt', 'w') as f:\n",
        "\n",
        "    for line in lines:\n",
        "        f.write(line)\n",
        "        f.write('\\n')\n",
        "\n",
        "\n",
        "  return(sum_dat)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ldeooD37Tkrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_check_fairness(dfr,target,model,test_size,sens,metric,pos,neg,threshold):\n",
        "  dat=preprocess(dfr,target)\n",
        "  dataf,acc=modelling(dat,target,model,test_size,sens)\n",
        "  tdata=compare_groups(dataf,sens,metric,target,'prediction',pos,neg)\n",
        "  if len(tdata)==0:\n",
        "    Conclusion='Model too simplistic'\n",
        "    sum_dat='No metrics calculated'\n",
        "  else:\n",
        "    sum_dat,prob_dat=threshold_eval(tdata,threshold,dataf,sens)\n",
        "    print('accuracy: ',acc)\n",
        "    #print(prob_dat)\n",
        "\n",
        "    \n",
        "\n",
        "    if len(sum_dat)==1:\n",
        "      Conclusion='Model too simplistic'\n",
        "    \n",
        "    elif len(prob_dat)==0:\n",
        "      Conclusion='The given model is deemed to be fair towards all levels of '+sens+' at the '+str(threshold*100)+' % level of significance'\n",
        "\n",
        "    else:\n",
        "      Conclusion='The given model is deemed to be unfair towards the following levels of '+sens+' at the '+str(threshold*100)+' % level of significance : '\n",
        "      kk=0\n",
        "      for ii in prob_dat['level']:\n",
        "        kk+=1\n",
        "        Conclusion+=ii\n",
        "        if kk<len(prob_dat):\n",
        "          Conclusion+=', '\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(Conclusion)\n",
        "\n",
        "  lines = ['The Following are the parameters of the fairness-test: ', 'Target variable: '+target,'Model used: '+model,'Ratio of Test Set Split: '+str(int(test_size*100))+' %','Sensitive Variable: '+sens,\n",
        "           'Metric used for calculating parity(fairness): '+metric.__name__,'Level of Significance: '+(str(int(threshold*100)))+' %',Conclusion,'Accuracy of the model: '+str(acc)]\n",
        "  with open('report.txt', 'w') as f:\n",
        "\n",
        "    for line in lines:\n",
        "        f.write(line)\n",
        "        f.write('\\n')\n",
        "\n",
        "  \n",
        "  return(sum_dat)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FveF_TL5TH9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_check_fairness_multiclass(dfr,target,model,test_size,sens,metric,reference,threshold):\n",
        "  dat=preprocess(dfr,target)\n",
        "  dataf,acc=modelling(dat,target,model,test_size,sens)\n",
        "  tdata=compare_groups_multiclass(dataf,sens,metric,reference,target,'prediction')\n",
        "  if len(tdata)==0:\n",
        "    Conclusion='Model too simplistic'\n",
        "    sum_dat='No metrics calculated'\n",
        "  else:\n",
        "    sum_dat,prob_dat=threshold_eval(tdata,threshold,dataf,sens)\n",
        "    print('accuracy: ',acc)\n",
        "    #print(prob_dat)\n",
        "\n",
        "    \n",
        "\n",
        "    if len(sum_dat)==1:\n",
        "      Conclusion='Model too simplistic'\n",
        "    \n",
        "    elif len(prob_dat)==0:\n",
        "      Conclusion='The given model is deemed to be fair towards all levels of '+sens+' at the '+str(threshold*100)+' % level of significance'\n",
        "\n",
        "    else:\n",
        "      Conclusion='The given model is deemed to be unfair towards the following levels of '+sens+' at the '+str(threshold*100)+' % level of significance : '\n",
        "      kk=0\n",
        "      for ii in prob_dat['level']:\n",
        "        kk+=1\n",
        "        Conclusion+=ii\n",
        "        if kk<len(prob_dat):\n",
        "          Conclusion+=', '\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(Conclusion)\n",
        "\n",
        "  lines = ['The Following are the parameters of the fairness-test: ', 'Target variable: '+target,'Model used: '+model,'Ratio of Test Set Split: '+str(int(test_size*100))+' %','Sensitive Variable: '+sens,\n",
        "           'Metric used for calculating parity(fairness): '+metric.__name__,'Level of Significance: '+(str(int(threshold*100)))+' %',Conclusion,'Accuracy of the model: '+str(acc)]\n",
        "  with open('report.txt', 'w') as f:\n",
        "\n",
        "    for line in lines:\n",
        "        f.write(line)\n",
        "        f.write('\\n')\n",
        "\n",
        "  \n",
        "  return(sum_dat)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wtvTbi2SIlhW"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}